{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reviews_decoded'], dtype='object')\n",
      "Index(['reviews_decoded'], dtype='object')\n",
      "Index(['sentiment'], dtype='object')\n",
      "Index(['sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    from transformers import DistilBertTokenizer\n",
    "\n",
    "    X_train = pd.read_csv(\"../data/Train_Test_splits/X_train_50proc_trunc_pad.csv\")\n",
    "    X_test = pd.read_csv(\"../data/Train_Test_splits/X_test_50proc_trunc_pad.csv\")\n",
    "    y_train = pd.read_csv(\"../data/Train_Test_splits/y_train_50proc.csv\")\n",
    "    y_test = pd.read_csv(\"../data/Train_Test_splits/y_test_50proc.csv\")\n",
    "\n",
    "    print(X_train.columns)\n",
    "    print(X_test.columns)\n",
    "    print(y_train.columns)\n",
    "    print(y_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No GPU found. Performance may be slower.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No GPU found. Performance may be slower.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\u\\.virtualenvs\\NLP_Project_AI23-3adb1o2j\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1690\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1690\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\u\\.virtualenvs\\NLP_Project_AI23-3adb1o2j\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1673\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1675\u001b[0m checks \u001b[38;5;241m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends)\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[1;31mImportError\u001b[0m: \nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import TFAutoModelForSequenceClassification, DistilBertTokenizer\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "\n",
    "    if not tf.config.list_physical_devices(\"GPU\"):\n",
    "        print(\"Warning: No GPU found. Performance may be slower.\")\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    test_dataset = dataset[\"test\"].select(range(100))\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    test_tf_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            np.array(test_dataset[\"input_ids\"]),\n",
    "            np.array(test_dataset[\"attention_mask\"]),\n",
    "            np.array(test_dataset[\"label\"]),\n",
    "        )\n",
    "    ).batch(8)\n",
    "\n",
    "    @tf.function\n",
    "    def predict_step(input_ids, attention_mask):\n",
    "        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        logits = model(inputs, training=False).logits\n",
    "        return np.argmax(logits.numpy(), axis=1)\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for input_ids, attention_mask, batch_labels in test_tf_dataset:\n",
    "        preds = predict_step(input_ids, attention_mask)\n",
    "        predictions.extend(preds)\n",
    "        labels.extend(batch_labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import tensorflow as tf\n",
    "    print(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u\\.virtualenvs\\NLP_Project_AI23-3adb1o2j\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\u\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\u\\.virtualenvs\\NLP_Project_AI23-3adb1o2j\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\u\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 88851.98 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 187322.65 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 207313.88 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 154.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoModelForSequenceClassification, DistilBertTokenizer\n",
    "    import torch\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    ).to(device)\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    test_dataset = dataset[\"test\"].select(range(100))\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "        )\n",
    "\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    inputs = torch.tensor(test_dataset[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(test_dataset[\"attention_mask\"]).to(device)\n",
    "    labels = torch.tensor(test_dataset[\"label\"]).to(device)\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(inputs), 8):\n",
    "            batch_input_ids = inputs[i : i + 8]\n",
    "            batch_attention_mask = attention_mask[i : i + 8]\n",
    "            batch_labels = labels[i : i + 8]\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(tf.config.list_physical_devices(\"GPU\"))\n",
    "    devices = tf.config.list_physical_devices()\n",
    "    print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU is NOT available.\n",
      "\n",
      "Matrix multiplication done on GPU!\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    device = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "    device_string = \"GPU is available!\" if device else \"GPU is NOT available.\"\n",
    "    print()\n",
    "    print(device_string)\n",
    "    print()\n",
    "\n",
    "    # Somehow this takes unbelievably long time...\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        a = tf.random.normal([10, 10])\n",
    "        b = tf.random.normal([10, 10])\n",
    "        c = tf.matmul(a, b)\n",
    "\n",
    "        print(\"Matrix multiplication done on GPU!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Project_AI23-3adb1o2j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
